

- 우분투 원격지에서서 vscode 연결하기 
    vscode 에서 좌하단 꺽쇠표시 눌러서 호스트 등록
    ssh sixtick@192.168.0.16       // LLM 서버로 활용할 예정임
    패스워드 입력
    폴더 오픈 /home/sixtick/venv/source/recommend_movie/ 
    패스워드 입력

- 테스트하기 
    ssh sixtick@192.168.0.16                             // 원격접속
    source venv/bin/activate                             // 파이썬 가상환경 켜기
    cd venv/source/recommend_movie/llmapi/               //실행코드로 이동
    uvicorn main:app --reload --host 0.0.0.0 --port 18000    // 서버 실행         
    http://192.168.0.16:18000/llm                      // 브라우저에서 테스트
    http://sixtick.duckdns.org:19821/llm?role=사용자가 제시한 숫자들 중에 제일 큰 숫자를 말해줘&query=1,2,3,4,10,6,7,8,9

- 파이썬 환경
    source ~/venv/bin/activate     // 가상환경 활성화. pip 실행하려면 가상환경에서 해야함. 

- vscode
    명령 팔렛트를 열어 git:clone 을 실행한다. 

- fastAPI (우분투 환경)
    파이썬 기반의 경량 api 서버. 파이썬 3.7 이상에서 동작한다. (python3 --version 3.12.3)
    pip install "fastapi[all]"    //  의존성 체크하면서 설치함. uvicorn 포함 이것저것
    uvicorn main:app --reload       // 앱 실행 (main.py의 app 객체를 실행하는것임. --reload 옵션은 코드변경시 서버를 자동으로 재시작함)
    기본적으로 127.0.0.1:8000 에서 시작됨.
    uvicorn main:app --reload --host 0.0.0.0 --port 18000    // 실제 운영할 주소, 명령어

    
- 방화벽 열기
    sudo iptables --list
    sudo iptables -I INPUT 1 -p tcp --dport 18000 -j ACCEPT
    sudo apt install iptables-persistent     
    sudo iptables-save | sudo tee /etc/iptables/rules.v4
    sudo ip6tables-save | sudo tee /etc/iptables/rules.v6
    ss -tuln


- LLM 모델
    https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B          // 양자화 모델
    https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1        // 모델 및 프롬프트 형식


- 라마 환경설정
# 리눅스
# sudo apt install curl
# curl -fsSL https://ollama.com/install.sh | sh

# ollama pull llama3.1
# ollama run llama3.1

# sudo apt update
# sudo apt install build-essential cmake                 ## llama_cpp_python 설치할때 c++파일을 빌드하면서 설치한다. 그래서 cmake를 필요로 한다. 
# pip install transformers langchain langchain-community vllm llama_cpp_python                        ## 3.1

# 우선 모델을 다운받는다 .gguf 파일
# ############# huggingface-cli download MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M
# ############# Download complete. Moving file to /home/sixtick/.cache/huggingface/hub/models--MLP-KTLim--llama-3-Korean-Bllossom-8B-gguf-Q4_K_M/blobs/fcb8006481e20379e4369a3169747b11fbad7038
# huggingface-cli download MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M --local-dir='/home/sixtick/ollama/'
# echo "FROM ./llama-3-Korean-Bllossom-8B-Q4_K_M.gguf" > Modelfile
# ollama create kor8b -f Modelfile
# ollama run kor8b

# 3.2 버전 3B 한국어 버전
# huggingface-cli download Bllossom/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M --local-dir='/home/sixtick/ollama/'
# cd /home/sixtick/ollama/
# echo "FROM ./llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf" > Modelfile
# ollama create kor3b -f Modelfile
# ollama run kor3b
